---
title: "Deep Learning based classification of GW sources."
publishedAt: "2025-08-22"
summary: "Explored Deep Learning based classification and parameter estimation of various kinds of Gravitational Wave sources."
team:
  - name: "Saqlain Afroz"
    role: "Student"
    avatar: "/images/avatar.jpg"
    linkedIn: "https://www.linkedin.com/company/once-ui/"
  - name: "Dr. Apratim Ganguly"
    role: "Scietific Officer"
    avatar: "/images/projects/project-01/apratim.jpg"
    linkedIn: "https://www.linkedin.com/company/once-ui/"
  - name: "Dr. Anuj Mishra"
    role: "Post Doc ICTS"
    avatar: "/images/projects/project-01/anuj.jpg"
    linkedIn: "https://www.linkedin.com/company/once-ui/"
link: "https://github.com/AfrozSaqlain/Eccentricity_vs_Lensed"
---

# ğŸ”­ GWTorch Project Overview

A machine learning toolkit to classify gravitational wave (GW) signals â€” eccentric, lensed, and non-eccentric â€” using CNN and Transformer models. This project involves generation, preprocessing, training, and evaluation of GW signals using Q-transform spectrograms.

To install the package just: `bash install.sh`

# A guide to File Structure

<pre>
.
â”œâ”€â”€ environment.yml
â”œâ”€â”€ install.sh
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ gwtorch
â”‚Â Â  â”œâ”€â”€ inference
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cnn_eval.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ transformer_eval.py
â”‚Â Â  â”œâ”€â”€ modules
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ constants.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ general_utils.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ gw_utils.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ml_utils.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ neural_net.py
â”‚Â Â  â”œâ”€â”€ training
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ binary_cnn_classifier.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ binary_transformer_classifier.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cnn.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Meta_model.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ transformer.py
â”‚Â Â  â””â”€â”€ waveform_generation
â”‚Â Â      â”œâ”€â”€ gen_ln_B.py
â”‚Â Â      â”œâ”€â”€ gen.py
â”‚Â Â      â”œâ”€â”€ exp_gen.py
â”‚Â Â      â”œâ”€â”€ Prior_Files
â”‚Â Â      â””â”€â”€ __pycache__
â””â”€â”€ README.md

</pre>

## ğŸ“Œ Key Files Description

- `exp_gen.py`: Generates Q-transform spectrograms of synthetic GW signals.
- `cnn.py`, `transformer.py`: Contain CNN and Transformer model definitions with training/testing routines.
- `cnn_eval.py`, `transformer_eval.py`: Specifically used to evaluate trained CNN and Transformer models on test data.


# ğŸ–¥ï¸ Command Line Interface (CLI)

- **`exp_gen_gwtorch --num-samples [int] --path-name [str] --batch-size [int]`** : Used to generate qtransform of the waveform samples.
- **`cnn_train_gwtorch --batch_size [default: 128] --epochs [default: 20] --lr [default: 3e-4] --gamma [default: 0.7] --model_path [default: cnn_model0.pth]  --k-folds [default: 5] --use-kfold [stores Truth value]`**: Train the CNN model on the data generated using previous code. The directory `models` is created if it doesn't exist.
- **`transformer_train_gwtorch --model [default: ViT] --batch_size [default: 512] --epochs [default 20] --lr [default: 3e-5] --gamma [default: 0.7] --seed [default: 42] --k-folds [default: 5] --use-kfold [stores Truth value]`**:  Train the Transformer model on the data generated.

# ğŸŒŠ How GW waveforms are generated

We use `TEOBResumS` to generate signals assuming `f_lower` = 5. Two sets of data are generated: 
- Eccentric: By giving the `eccentricity` parameter, sampled from prior assuming `Uniform` distribution in [0.1, 0.6].
- Non-Eccentric: By giving the `eccentricity` parameter value to be equal to 0.

The Non-Eccentric waveform is then used to generate unlensed data as well as lensed data. Lensing is done using `GWMAT` package.

The waveforms are then projected onto given set of detectors by using `ra`, `dec`, and `polarization` values sampled from the prior

Each wave is then tapered. Then we generated noise from corresponding PSD.

Next we pad each GW signal and add it to the noise such that the peak of the signal lies within 2.2 seconds to 2 seconds window before the noise ends. This is done to make the training process robust to moderate time
translations in the signal.

The signal is then cropped so that the data is of 8s duration.

Then finally we generate the `q_transforms` of each signal and save them as `.npy` file with naming convention such that if the signal is `lensed` then the file is located at `lensed / {num}.npy`, where `num` just represnts file number.

## ğŸ“‹ Prior Distributions Table

<Table
  data={{
    headers: [
      { content: "Parameter", key: "parameter", sortable: true },
      { content: "Distribution Type", key: "distribution_type", sortable: true },
      { content: "Range / Description", key: "range_description" }
    ],
    rows: [
      ["mass1", "Constraint", "[10, 100]"],
      ["mass2", "Constraint", "[10, 100]"],
      ["mass_ratio", "UniformInComponentsMassRatio", "[0.1, 1]"],
      ["chirp_mass", "UniformInComponentsChirpMass", "[10, 100]"],
      ["spin1z", "Uniform", "[-0.9, 0.9]"],
      ["spin2z", "Uniform", "[-0.9, 0.9]"],
      ["eccentricity", "Uniform", "[0.1, 0.6]"],
      ["coa_phase", "Uniform", "[0.0, 2Ï€]"],
      ["SNR", "PowerLaw (Î± = -4)", "[10, 500]"],
      ["dec", "Cosine", "[âˆ’Ï€/2, Ï€/2]"],
      ["ra", "Uniform (periodic)", "[0, 2Ï€]"],
      ["polarization", "Uniform (periodic)", "[0, Ï€]"],
      ["Log_Mlz", "Uniform", "[1, 5]"],
      ["yl", "PowerLaw (Î± = 1)", "[0.01, 1.0]"]
    ]
  }}
/>

**Note:** The `exp_gen.py` script also generates a Lookup table for parameter values corresponding to each sample and each category. Also sample `data` generated is given in `data_for_reference` folder alongwith its `Parameter Reference` in csv format.